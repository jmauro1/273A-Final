# -*- coding: utf-8 -*-
"""

This file generates a network with 2 linear layers in order to perform classification on the MNIST dataset. The code compares results when the Network
contains batch normalization, weight normalization, batch and weight normalization, and no normalization.

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h9-l290F5WloFQq6U_Xgls9NBl1yNA_8
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from random import randint
import time
import torchvision
import torchvision.datasets as datasets
from torch.nn.utils import weight_norm

from google.colab import files

import matplotlib.pyplot as plt
import numpy as np

#Display GPU Info
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

#Memory Management
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

def display_num_param(net): #display parameters in network
    nb_param = 0
    for param in net.parameters():
        nb_param += param.numel()
    print('There are {} ({:.2f} million) parameters in this neural network'.format(
        nb_param, nb_param/1e6)
         )

def get_error(scores, labels): #calculate accuracy given true labels and network output

    bs = scores.size(0)
    predicted_labels = scores.argmax(dim=1)
    indicator = (predicted_labels == labels)
    num_matches=indicator.sum()

    return 1-num_matches.float()/bs

device = torch.device("cuda")

#Download MNIST data
mnist_train = datasets.MNIST(root = './data', train = True, download = True, transform = None)
mnist_test = datasets.MNIST(root = './data', train = False, download = True, transform = None)

train_data = mnist_train.data
train_label = mnist_train.targets
test_data = mnist_test.data
test_label = mnist_test.targets

train_data = train_data.to(device, dtype=torch.float32)
train_label = train_label.to(device, dtype=torch.long)

test_data = test_data.to(device, dtype=torch.float32)
test_label = test_label.to(device, dtype=torch.long)

class five_layer_linear_net(nn.Module):

    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, hidden_size4, output_size, weight_normalization = True):

        super().__init__()

        self.layer1 = nn.Linear(input_size, hidden_size1, bias = True)
        self.batch_norm1 = nn.BatchNorm1d(hidden_size1)
        self.layer2 = nn.Linear(hidden_size1, hidden_size2, bias = True)
        self.batch_norm2 = nn.BatchNorm1d(hidden_size2)
        self.layer3 = nn.Linear(hidden_size2, hidden_size3, bias = True)
        self.batch_norm3 = nn.BatchNorm1d(hidden_size3)
        self.layer4 = nn.Linear(hidden_size3, hidden_size4, bias = True)
        self.batch_norm4 = nn.BatchNorm1d(hidden_size4)
        self.layer5 = nn.Linear(hidden_size4, output_size, bias = True)

        if weight_normalization:
          self.layer1 = weight_norm(self.layer1)
          self.layer2 = weight_norm(self.layer2)
          self.layer3 = weight_norm(self.layer3)
          self.layer4 = weight_norm(self.layer4)
          self.layer5 = weight_norm(self.layer5)



    def forward(self, x, batch_norm = True):

      if batch_norm:
        x = self.layer1(x)
        x = self.batch_norm1(x)
        x = F.relu(x)
        x = self.layer2(x)
        x = self.batch_norm2(x)
        x = F.relu(x)
        x = self.layer3(x)
        x = self.batch_norm3(x)
        x = F.relu(x)
        x = self.layer4(x)
        x = self.batch_norm4(x)
        x = F.relu(x)
        x = self.layer5(x)

      else:
        x = self.layer1(x)
        x = F.relu(x)
        x = self.layer2(x)
        x = F.relu(x)
        x = self.layer3(x)
        x = F.relu(x)
        x = self.layer4(x)
        x = F.relu(x)
        x = self.layer5(x)

      return x



mean = train_data.mean()
std= train_data.std()

mean = mean.to(device)
std = std.to(device)

criterion = nn.CrossEntropyLoss()
my_lr=0.25
bs= 100

num_epochs = 20

def eval_on_test_set(batch_norm = True):

    running_error=0
    num_batches=0

    with torch.no_grad():

        for i in range(0,10000,bs):

            minibatch_data =  test_data[i:i+bs]
            minibatch_label= test_label[i:i+bs]

            minibatch_data=minibatch_data.to(device)
            minibatch_label=minibatch_label.to(device)


            inputs = minibatch_data.view(-1, 28*28)

            inputs = (inputs - mean)/std

            scores=net( inputs, batch_norm )

            error = get_error( scores , minibatch_label)

            running_error += error.item()

            num_batches+=1


    total_error = running_error/num_batches

    print( 'error rate on test set =', total_error*100 ,'percent')

    return total_error

def train_the_net(my_lr, batch_norm = True):

  train_errors = []
  test_errors = []

  start = time.time()

  for epoch in range(1,num_epochs):

      if epoch==5 or epoch == 8 or epoch==11:
          my_lr = my_lr / 2

      optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )

      running_loss=0
      running_error=0
      num_batches=0

      shuffled_indices=torch.randperm(60000)

      for count in range(0,60000,bs):

          optimizer.zero_grad()

          indices = shuffled_indices[count:count+bs]
          minibatch_data = train_data[indices]
          minibatch_label = train_label[indices]

          minibatch_data = minibatch_data.to(device)
          minibatch_label = minibatch_label.to(device)

          inputs = minibatch_data.view(-1, 28*28)

          inputs = (inputs - mean)/std

          inputs.requires_grad_(True)

          scores=net( inputs, batch_norm )

          #scores = scores + .01

          loss = criterion( scores , minibatch_label)

          loss.backward()

          optimizer.step()


          num_batches+=1
          with torch.no_grad():
              running_loss += loss.item()
              error = get_error( scores , minibatch_label)
              running_error += error.item()


      total_loss = running_loss/num_batches
      total_error = running_error/num_batches
      elapsed = (time.time()-start)/60

      train_errors.append(100*total_error)

      print('epoch=',epoch, '\t time=', elapsed,'min', '\t lr=', my_lr  ,'\t loss=', total_loss , '\t error=', total_error*100 ,'percent')
      test_error = eval_on_test_set(batch_norm)
      test_errors.append(100*test_error)
      print(' ')

  return train_errors, test_errors

#Run network weight weight and batch norm
net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = True)
net = net.to(device)
batch_weight_norm_train_errors, batch_weight_norm_test_errors = train_the_net(my_lr, batch_norm = True)





#Run network weight weight norm
net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = True)
net = net.to(device)
weight_norm_train_errors, weight_norm_test_errors = train_the_net(my_lr, batch_norm = False)

#Run network weight batch norm
net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = False)
net = net.to(device)
batch_norm_train_errors, batch_norm_test_errors = train_the_net(my_lr, batch_norm = True)

#Run network weight no norm
net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = False)
net = net.to(device)
no_norm_train_errors, no_norm_test_errors = train_the_net(my_lr, batch_norm = False)



#Create and save plots

epoch_axis = np.arange(1, num_epochs)

plt.plot(epoch_axis, no_norm_train_errors,'sk-' , label = 'None')
plt.plot(epoch_axis, batch_norm_train_errors, '^b-', label = "BN")
plt.plot(epoch_axis, weight_norm_train_errors, 'dg-', label = 'WN')
plt.plot(epoch_axis, batch_weight_norm_train_errors, 'or-', label = "BN + WN")
plt.legend()
plt.xticks(np.arange(0, 20, step=2))
plt.xlabel("Epoch")
plt.ylabel("Train Error %")
plt.title("Error on MNIST Training Data 5 Layer Network")
plt.savefig("5layerTrain.png")
files.download("5layerTrain.png")
plt.show()

plt.plot(epoch_axis, no_norm_test_errors,'sk-' , label = 'None')
plt.plot(epoch_axis, batch_norm_test_errors, '^b-', label = "BN")
plt.plot(epoch_axis, weight_norm_test_errors, 'dg-', label = 'WN')
plt.plot(epoch_axis, batch_weight_norm_test_errors, 'or-', label = "BN + WN")
plt.legend()
plt.xticks(np.arange(0, 20, step=2))
plt.xlabel("Epoch")
plt.ylabel("Test Error %")
plt.title("Error on MNIST Test Data 5 Layer Network")
plt.savefig("5layerTest.png")
files.download("5layerTest.png")
plt.show()
