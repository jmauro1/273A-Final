# -*- coding: utf-8 -*-
"""

This file generates a network with 2 linear layers in order to perform classification on the MNIST dataset. The code compares results when the Network
contains batch normalization, weight normalization, batch and weight normalization, and no normalization.

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15OCpqba504iD3YKXA9AbY8HNodwIzjOT
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from random import randint
import time
import torchvision
import torchvision.datasets as datasets
from torch.nn.utils import weight_norm

from google.colab import files

import matplotlib.pyplot as plt
import numpy as np

#Display GPU Info
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

#Memory Management
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

def display_num_param(net): #display number of parameters in network
    nb_param = 0
    for param in net.parameters():
        nb_param += param.numel()
    print('There are {} ({:.2f} million) parameters in this neural network'.format(
        nb_param, nb_param/1e6)
         )

def get_error(scores, labels): #calculate error given network output and true labels

    bs = scores.size(0)
    predicted_labels = scores.argmax(dim=1)
    indicator = (predicted_labels == labels)
    num_matches=indicator.sum()

    return 1-num_matches.float()/bs

device = torch.device("cuda")

#Download MNIST data
mnist_train = datasets.MNIST(root = './data', train = True, download = True, transform = None)
mnist_test = datasets.MNIST(root = './data', train = False, download = True, transform = None)

train_data = mnist_train.data
train_label = mnist_train.targets
test_data = mnist_test.data
test_label = mnist_test.targets

train_data = train_data.to(device, dtype=torch.float32)
train_label = train_label.to(device, dtype=torch.long)
#train_label = train_label.to(device, dtype=torch.float32)
test_data = test_data.to(device, dtype=torch.float32)
test_label = test_label.to(device, dtype=torch.long)
#test_label = test_label.to(device, dtype=torch.float32)

class ten_layer_linear_net(nn.Module):

    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, hidden_size4, hidden_size5, hidden_size6, hidden_size7, hidden_size8, hidden_size9, output_size, weight_normalization = True):

        super().__init__()

        self.layer1 = nn.Linear(input_size, hidden_size1, bias = True)
        self.batch_norm1 = nn.BatchNorm1d(hidden_size1)
        self.layer2 = nn.Linear(hidden_size1, hidden_size2, bias = True)
        self.batch_norm2 = nn.BatchNorm1d(hidden_size2)
        self.layer3 = nn.Linear(hidden_size2, hidden_size3, bias = True)
        self.batch_norm3 = nn.BatchNorm1d(hidden_size3)
        self.layer4 = nn.Linear(hidden_size3, hidden_size4, bias = True)
        self.batch_norm4 = nn.BatchNorm1d(hidden_size4)
        self.layer5 = nn.Linear(hidden_size4, hidden_size5, bias = True)
        self.batch_norm5 = nn.BatchNorm1d(hidden_size5)
        self.layer6 = nn.Linear(hidden_size5, hidden_size6, bias = True)
        self.batch_norm6 = nn.BatchNorm1d(hidden_size6)
        self.layer7 = nn.Linear(hidden_size6, hidden_size7, bias = True)
        self.batch_norm7 = nn.BatchNorm1d(hidden_size7)
        self.layer8 = nn.Linear(hidden_size7, hidden_size8, bias = True)
        self.batch_norm8 = nn.BatchNorm1d(hidden_size8)
        self.layer9 = nn.Linear(hidden_size8, hidden_size9, bias = True)
        self.batch_norm9 = nn.BatchNorm1d(hidden_size9)
        self.layer10 = nn.Linear(hidden_size9, output_size, bias = True)

        if weight_normalization:
          self.layer1 = weight_norm(self.layer1)
          self.layer2 = weight_norm(self.layer2)
          self.layer3 = weight_norm(self.layer3)
          self.layer4 = weight_norm(self.layer4)
          self.layer5 = weight_norm(self.layer5)
          self.layer6 = weight_norm(self.layer6)
          self.layer7 = weight_norm(self.layer7)
          self.layer8 = weight_norm(self.layer8)
          self.layer9 = weight_norm(self.layer9)
          self.layer10 = weight_norm(self.layer10)



    def forward(self, x, batch_norm = True):

      if batch_norm:
        x = self.layer1(x)
        x = self.batch_norm1(x)
        x = F.relu(x)
        x = self.layer2(x)
        x = self.batch_norm2(x)
        x = F.relu(x)
        x = self.layer3(x)
        x = self.batch_norm3(x)
        x = F.relu(x)
        x = self.layer4(x)
        x = self.batch_norm4(x)
        x = F.relu(x)
        x = self.layer5(x)
        x = self.batch_norm5(x)
        x = self.layer6(x)
        x = self.batch_norm6(x)
        x = F.relu(x)
        x = self.layer7(x)
        x = self.batch_norm7(x)
        x = F.relu(x)
        x = self.layer8(x)
        x = self.batch_norm8(x)
        x = F.relu(x)
        x = self.layer9(x)
        x = self.batch_norm9(x)
        x = F.relu(x)
        x = self.layer10(x)

      else:
        x = self.layer1(x)
        x = F.relu(x)
        x = self.layer2(x)
        x = F.relu(x)
        x = self.layer3(x)
        x = F.relu(x)
        x = self.layer4(x)
        x = F.relu(x)
        x = self.layer5(x)
        x = F.relu(x)
        x = self.layer6(x)
        x = F.relu(x)
        x = self.layer7(x)
        x = F.relu(x)
        x = self.layer8(x)
        x = F.relu(x)
        x = self.layer9(x)
        x = F.relu(x)
        x = self.layer10(x)

      return x

mean = train_data.mean()
std= train_data.std()

mean = mean.to(device)
std = std.to(device)

criterion = nn.CrossEntropyLoss()
my_lr=0.25
bs= 100

num_epochs = 20

def eval_on_test_set(batch_norm = True): #evaluate network on testing data

    running_error=0
    num_batches=0

    with torch.no_grad():

        for i in range(0,10000,bs):

            minibatch_data =  test_data[i:i+bs]
            minibatch_label= test_label[i:i+bs]

            minibatch_data=minibatch_data.to(device)
            minibatch_label=minibatch_label.to(device)


            inputs = minibatch_data.view(-1, 28*28)

            inputs = (inputs - mean)/std

            scores=net( inputs, batch_norm)

            error = get_error( scores , minibatch_label)

            running_error += error.item()

            num_batches+=1


    total_error = running_error/num_batches

    print( 'error rate on test set =', total_error*100 ,'percent')

    return total_error

def train_the_net(my_lr, batch_norm = True): #train the network

  train_errors = []
  test_errors = []

  start = time.time()

  for epoch in range(1,num_epochs):

      if epoch==5 or epoch == 8 or epoch==11:
          my_lr = my_lr / 2

      optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )

      running_loss=0
      running_error=0
      num_batches=0

      shuffled_indices=torch.randperm(60000)

      for count in range(0,60000,bs):

          optimizer.zero_grad()

          indices = shuffled_indices[count:count+bs]
          minibatch_data = train_data[indices]
          minibatch_label = train_label[indices]

          minibatch_data = minibatch_data.to(device)
          minibatch_label = minibatch_label.to(device)

          inputs = minibatch_data.view(-1, 28*28)

          inputs = (inputs - mean)/std

          inputs.requires_grad_(True)

          scores=net( inputs, batch_norm )

          #scores = scores + .01

          loss = criterion( scores , minibatch_label)

          loss.backward()

          optimizer.step()


          num_batches+=1
          with torch.no_grad():
              running_loss += loss.item()
              error = get_error( scores , minibatch_label)
              running_error += error.item()


      total_loss = running_loss/num_batches
      total_error = running_error/num_batches
      elapsed = (time.time()-start)/60

      train_errors.append(100*total_error)

      print('epoch=',epoch, '\t time=', elapsed,'min', '\t lr=', my_lr  ,'\t loss=', total_loss , '\t error=', total_error*100 ,'percent')
      test_error = eval_on_test_set(batch_norm)
      test_errors.append(100*test_error)
      print(' ')

  return train_errors, test_errors

#Run network with weight and batch norm
net = ten_layer_linear_net(784, 250, 500, 500, 750, 500, 250, 500, 400, 750, 10, weight_normalization = True)
net = net.to(device)
batch_weight_norm_train_errors, batch_weight_norm_test_errors = train_the_net(my_lr, batch_norm = True)

#Run network with weight norm
net = ten_layer_linear_net(784, 250, 500, 500, 750, 500, 250, 500, 400, 750, 10, weight_normalization = True)
net = net.to(device)
weight_norm_train_errors, weight_norm_test_errors = train_the_net(my_lr, batch_norm = False)

#Run network with batch norm
net = ten_layer_linear_net(784, 250, 500, 500, 750, 500, 250, 500, 400, 750, 10, weight_normalization = False)
net = net.to(device)
batch_norm_train_errors, batch_norm_test_errors = train_the_net(my_lr, batch_norm = True)

#Run network with no norm
net = ten_layer_linear_net(784, 250, 500, 500, 750, 500, 250, 500, 400, 750, 10, weight_normalization = False)
net = net.to(device)
no_norm_train_errors, no_norm_test_errors = train_the_net(my_lr, batch_norm = False)

#Create and save plots



epoch_axis = np.arange(1, num_epochs)

plt.plot(epoch_axis, no_norm_train_errors,'sk-' , label = 'None')
plt.plot(epoch_axis, batch_norm_train_errors, '^b-', label = "BN")
plt.plot(epoch_axis, weight_norm_train_errors, 'dg-', label = 'WN')
plt.plot(epoch_axis, batch_weight_norm_train_errors, 'or-', label = "BN + WN")
plt.legend()
plt.xticks(np.arange(0, 20, step=2))
plt.xlabel("Epoch")
plt.ylabel("Train Error %")
plt.title("Error on MNIST Training Data 10 Layer Network")
plt.savefig("10layerTrain.png")
files.download("10layerTrain.png")
plt.show()

plt.plot(epoch_axis, no_norm_test_errors,'sk-' , label = 'None')
plt.plot(epoch_axis, batch_norm_test_errors, '^b-', label = "BN")
plt.plot(epoch_axis, weight_norm_test_errors, 'dg-', label = 'WN')
plt.plot(epoch_axis, batch_weight_norm_test_errors, 'or-', label = "BN + WN")
plt.legend()
plt.xticks(np.arange(0, 20, step=2))
plt.xlabel("Epoch")
plt.ylabel("Test Error %")
plt.title("Error on MNIST Test Data 10 Layer Network")
plt.savefig("10layerTest.png")
files.download("10layerTest.png")
plt.show()
