# -*- coding: utf-8 -*-
"""
This file generates a five layer netwwork with 5 linear layers in order to perform classification on the MNIST dataset. The code compares results when the Network
contains batch normalization, weight normalization, batch and weight normalization, and no normalization. The weights of the network are initialized using the
standard uniform distribution, standard Gaussian distribution, Xavier uniform distribution, and a modified Gaussian dist.

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kvSMCKH0YVbJFKSnrwr91Bp2cwXPEB52
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from random import randint
import time
import torchvision
import torchvision.datasets as datasets
from torch.nn.utils import weight_norm

from google.colab import files
import math

#Display GPU Information if connected
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

#Memory Management
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

def display_num_param(net): #Displays number of parameters in network
    nb_param = 0
    for param in net.parameters():
        nb_param += param.numel()
    print('There are {} ({:.2f} million) parameters in this neural network'.format(
        nb_param, nb_param/1e6)
         )

def get_error(scores, labels): #calculate error from network output and groundtruth labels

    bs = scores.size(0)
    predicted_labels = scores.argmax(dim=1)
    indicator = (predicted_labels == labels)
    num_matches=indicator.sum()

    return 1-num_matches.float()/bs

device = torch.device("cuda")

#Download MNIST data
mnist_train = datasets.MNIST(root = './data', train = True, download = True, transform = None)
mnist_test = datasets.MNIST(root = './data', train = False, download = True, transform = None)

train_data = mnist_train.data
train_label = mnist_train.targets
test_data = mnist_test.data
test_label = mnist_test.targets

train_data = train_data.to(device, dtype=torch.float32)
train_label = train_label.to(device, dtype=torch.long)
test_data = test_data.to(device, dtype=torch.float32)
test_label = test_label.to(device, dtype=torch.long)

class five_layer_linear_net(nn.Module): #Define network class

    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, hidden_size4, output_size, weight_normalization = True, Gaussian_init_stand = False,Gaussian_init_modified = False, uniform_init_modified = False):
      """
      Creates a five layer linear network
      weight_normalization specifies if weights should be dynamically normalized
      Gaussian_init_stand specifies if weights should be initialized by being sampled from Gaussian dist. with 0 mean and sd. 1
      Gaussian_init_stand specifies if weights should be initialized by being sampled from Gaussian dist. with 0 mean and sd. (2/n)^.5
      uniform_init_modified specifies if weights should be initialized by being sampled from Xavier Uniform dist.


      """

        super().__init__()

        if Gaussian_init_stand:
          self.layer1 = nn.Linear(input_size, hidden_size1, bias = True)
          nn.init.normal_(self.layer1.weight, 0, 1/(5**.5))
          self.batch_norm1 = nn.BatchNorm1d(hidden_size1)
          self.layer2 = nn.Linear(hidden_size1, hidden_size2, bias = True)
          nn.init.normal_(self.layer2.weight, 0, 1/(5**.5))
          self.batch_norm2 = nn.BatchNorm1d(hidden_size2)
          self.layer3 = nn.Linear(hidden_size2, hidden_size3, bias = True)
          nn.init.normal_(self.layer3.weight, 0, 1/(5**.5))
          self.batch_norm3 = nn.BatchNorm1d(hidden_size3)
          self.layer4 = nn.Linear(hidden_size3, hidden_size4, bias = True)
          nn.init.normal_(self.layer4.weight, 0, 1/(5**.5))
          self.batch_norm4 = nn.BatchNorm1d(hidden_size4)
          self.layer5 = nn.Linear(hidden_size4, output_size, bias = True)
          nn.init.normal_(self.layer5.weight, 0, 1/(5**.5))

        elif Gaussian_init_modified:
          self.layer1 = nn.Linear(input_size, hidden_size1, bias = True)
          nn.init.normal_(self.layer1.weight, 0, (2/(self.layer1.weight.shape[0]*self.layer1.weight.shape[1]))**.5)
          self.batch_norm1 = nn.BatchNorm1d(hidden_size1)
          self.layer2 = nn.Linear(hidden_size1, hidden_size2, bias = True)
          nn.init.normal_(self.layer2.weight, 0, (2/(self.layer2.weight.shape[0]*self.layer2.weight.shape[1]))**.5)
          self.batch_norm2 = nn.BatchNorm1d(hidden_size2)
          self.layer3 = nn.Linear(hidden_size2, hidden_size3, bias = True)
          nn.init.normal_(self.layer3.weight, 0, (2/(self.layer3.weight.shape[0]*self.layer3.weight.shape[1]))**.5)
          self.batch_norm3 = nn.BatchNorm1d(hidden_size3)
          self.layer4 = nn.Linear(hidden_size3, hidden_size4, bias = True)
          nn.init.normal_(self.layer4.weight, 0, (2/(self.layer4.weight.shape[0]*self.layer4.weight.shape[1]))**.5)
          self.batch_norm4 = nn.BatchNorm1d(hidden_size4)
          self.layer5 = nn.Linear(hidden_size4, output_size, bias = True)
          nn.init.normal_(self.layer5.weight, 0, (2/(self.layer5.weight.shape[0]*self.layer5.weight.shape[1]))**.5)

        elif uniform_init_modified:
          self.layer1 = nn.Linear(input_size, hidden_size1, bias = True)
          nn.init.xavier_uniform_(self.layer1.weight)
          self.batch_norm1 = nn.BatchNorm1d(hidden_size1)
          self.layer2 = nn.Linear(hidden_size1, hidden_size2, bias = True)
          nn.init.xavier_uniform_(self.layer2.weight)
          self.batch_norm2 = nn.BatchNorm1d(hidden_size2)
          self.layer3 = nn.Linear(hidden_size2, hidden_size3, bias = True)
          nn.init.xavier_uniform_(self.layer3.weight)
          self.batch_norm3 = nn.BatchNorm1d(hidden_size3)
          self.layer4 = nn.Linear(hidden_size3, hidden_size4, bias = True)
          nn.init.xavier_uniform_(self.layer4.weight)
          self.batch_norm4 = nn.BatchNorm1d(hidden_size4)
          self.layer5 = nn.Linear(hidden_size4, output_size, bias = True)
          nn.init.xavier_uniform_(self.layer5.weight)

        else:
          self.layer1 = nn.Linear(input_size, hidden_size1, bias = True)
          self.batch_norm1 = nn.BatchNorm1d(hidden_size1)
          self.layer2 = nn.Linear(hidden_size1, hidden_size2, bias = True)
          self.batch_norm2 = nn.BatchNorm1d(hidden_size2)
          self.layer3 = nn.Linear(hidden_size2, hidden_size3, bias = True)
          self.batch_norm3 = nn.BatchNorm1d(hidden_size3)
          self.layer4 = nn.Linear(hidden_size3, hidden_size4, bias = True)
          self.batch_norm4 = nn.BatchNorm1d(hidden_size4)
          self.layer5 = nn.Linear(hidden_size4, output_size, bias = True)

        if weight_normalization:
          self.layer1 = weight_norm(self.layer1)
          self.layer2 = weight_norm(self.layer2)
          self.layer3 = weight_norm(self.layer3)
          self.layer4 = weight_norm(self.layer4)
          self.layer5 = weight_norm(self.layer5)



    def forward(self, x, batch_norm = True):

      if batch_norm:
        x = self.layer1(x)
        x = self.batch_norm1(x)
        x = F.relu(x)
        x = self.layer2(x)
        x = self.batch_norm2(x)
        x = F.relu(x)
        x = self.layer3(x)
        x = self.batch_norm3(x)
        x = F.relu(x)
        x = self.layer4(x)
        x = self.batch_norm4(x)
        x = F.relu(x)
        x = self.layer5(x)

      else:
        x = self.layer1(x)
        x = F.relu(x)
        x = self.layer2(x)
        x = F.relu(x)
        x = self.layer3(x)
        x = F.relu(x)
        x = self.layer4(x)
        x = F.relu(x)
        x = self.layer5(x)

      return x

mean = train_data.mean()
std= train_data.std()

mean = mean.to(device)
std = std.to(device)

criterion = nn.CrossEntropyLoss()
my_lr=0.25
bs= 100

num_epochs = 20

def eval_on_test_set(epoch, batch_norm = True): #Evaluate the network on the testing dataset

    running_error=0
    num_batches=0

    with torch.no_grad():

        for i in range(0,10000,bs):

            minibatch_data =  test_data[i:i+bs]
            minibatch_label= test_label[i:i+bs]

            minibatch_data=minibatch_data.to(device)
            minibatch_label=minibatch_label.to(device)


            inputs = minibatch_data.view(-1, 28*28)

            inputs = (inputs - mean)/std

            scores=net( inputs, batch_norm )

            error = get_error( scores , minibatch_label)

            running_error += error.item()

            num_batches+=1


    total_error = running_error/num_batches

    if epoch % 5 == 0:

      print( 'error rate on test set =', total_error*100 ,'percent')

    return total_error

def train_the_net(my_lr, batch_norm = True): #Train the network

  train_errors = []
  test_errors = []

  start = time.time()

  for epoch in range(1,num_epochs):

      if epoch==5 or epoch == 8 or epoch==11:
          my_lr = my_lr / 2

      optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )


      running_loss=0
      running_error=0
      num_batches=0

      shuffled_indices=torch.randperm(60000)

      for count in range(0,60000,bs):

          optimizer.zero_grad()

          indices = shuffled_indices[count:count+bs]
          minibatch_data = train_data[indices]
          minibatch_label = train_label[indices]

          minibatch_data = minibatch_data.to(device)
          minibatch_label = minibatch_label.to(device)

          inputs = minibatch_data.view(-1, 28*28)

          inputs = (inputs - mean)/std

          inputs.requires_grad_(True)


          scores = net( inputs, batch_norm )

          loss = criterion( scores , minibatch_label)

          loss.backward()


          optimizer.step()


          num_batches+=1
          with torch.no_grad():
              running_loss += loss.item()
              error = get_error( scores , minibatch_label)
              running_error += error.item()


      total_loss = running_loss/num_batches
      total_error = running_error/num_batches
      elapsed = (time.time()-start)/60

      train_errors.append(100*total_error)

      if epoch % 5 == 0:

        print('epoch=',epoch, '\t time=', elapsed,'min', '\t lr=', my_lr  ,'\t loss=', total_loss , '\t error=', total_error*100 ,'percent')
      test_error = eval_on_test_set(epoch, batch_norm)
      test_errors.append(100*test_error)
      print(' ')

  return train_errors, test_errors

#Run networks with batch and weight norm

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = True, Gaussian_init_stand = True)
net = net.to(device)
gaussStand_BW_norm_train_errors, gaussStand_BW_norm_test_errors = train_the_net(my_lr, batch_norm = True)

print(" ")

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = True, Gaussian_init_modified = True)
net = net.to(device)
gaussMod_BW_norm_train_errors, gaussMod_BW_norm_test_errors = train_the_net(my_lr, batch_norm = True)

print(" ")

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = True, uniform_init_modified = True)
net = net.to(device)
uniformMod_BW_norm_train_errors, uniformMod_BW_norm_test_errors = train_the_net(my_lr, batch_norm = True)

print(" ")

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = True)
net = net.to(device)
reg_BW_norm_train_errors, reg_BW_norm_test_errors = train_the_net(my_lr, batch_norm = True)

#Run networks with weight norm

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = True, Gaussian_init_stand = True)
net = net.to(device)
gaussStand_weight_norm_train_errors, gaussStand_weight_norm_test_errors = train_the_net(my_lr, batch_norm = False)

print(" ")

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = True, Gaussian_init_modified = True)
net = net.to(device)
gaussMod_weight_norm_train_errors, gaussMod_weight_norm_test_errors = train_the_net(my_lr, batch_norm = False)

print(" ")

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = True, uniform_init_modified = True)
net = net.to(device)
uniformMod_weight_norm_train_errors, uniformMod_weight_norm_test_errors = train_the_net(my_lr, batch_norm = False)

print(" ")

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = True)
net = net.to(device)
reg_weight_norm_train_errors, reg_weight_norm_test_errors = train_the_net(my_lr, batch_norm = False)

#Run networks with batch norm

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = False, Gaussian_init_stand = True)
net = net.to(device)
gaussStand_batch_norm_train_errors, gaussStand_batch_norm_test_errors = train_the_net(my_lr, batch_norm = True)

print(" ")

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = False, Gaussian_init_modified = True)
net = net.to(device)
gaussMod_batch_norm_train_errors, gaussMod_batch_norm_test_errors = train_the_net(my_lr, batch_norm = True)

print(" ")

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = False, uniform_init_modified = True)
net = net.to(device)
uniformMod_batch_norm_train_errors, uniformMod_batch_norm_test_errors = train_the_net(my_lr, batch_norm = True)

print(" ")

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = False)
net = net.to(device)
reg_batch_norm_train_errors, reg_batch_norm_test_errors = train_the_net(my_lr, batch_norm = True)

#Run networks with no norm

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = False, Gaussian_init_stand = True)
net = net.to(device)
gaussStand_no_norm_train_errors, gaussStand_no_norm_test_errors = train_the_net(my_lr, batch_norm = False)

print(" ")

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = False, Gaussian_init_modified = True)
net = net.to(device)
gaussMod_no_norm_train_errors, gaussMod_no_norm_test_errors = train_the_net(my_lr, batch_norm = False)

print(" ")

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = False, uniform_init_modified = True)
net = net.to(device)
uniformMod_no_norm_train_errors, uniformMod_no_norm_test_errors = train_the_net(my_lr, batch_norm = False)

print(" ")

net = five_layer_linear_net(784, 500, 750, 500, 250, 10, weight_normalization = False)
net = net.to(device)
reg_no_norm_train_errors, reg_no_norm_test_errors = train_the_net(my_lr, batch_norm = False)

import matplotlib.pyplot as plt
import numpy as np

epoch_axis = np.arange(1, num_epochs)

#Create and save plots

plt.plot(epoch_axis, gaussStand_no_norm_train_errors,'sk-' , label = 'Stand. Gaussian')
plt.plot(epoch_axis, gaussMod_no_norm_train_errors,'dg-' , label = 'Modified Gaussian')

plt.plot(epoch_axis, reg_no_norm_train_errors, '^b-', label = "Stand. Uniform Init")
plt.plot(epoch_axis, uniformMod_no_norm_train_errors,'pm-' , label = 'Modified Uniform')

plt.xticks(np.arange(0, 20, step=2))
plt.xlabel("Epoch")
plt.ylabel("Train Error %")
plt.title("Error on MNIST Training Data 5 Layer Network No Normalization")
plt.legend(title = "Initialization Method")
plt.savefig("NoNorm5layerTrain.png")
files.download("NoNorm5layerTrain.png")
plt.show()

plt.plot(epoch_axis, gaussStand_batch_norm_train_errors,'sk-' , label = 'Stand. Gaussian')
plt.plot(epoch_axis, gaussMod_batch_norm_train_errors,'dg-' , label = 'Modified Gaussian')

plt.plot(epoch_axis, reg_batch_norm_train_errors, '^b-', label = "Stand. Uniform Init")
plt.plot(epoch_axis, uniformMod_batch_norm_train_errors,'pm-' , label = 'Modified Uniform')

plt.xticks(np.arange(0, 20, step=2))
plt.xlabel("Epoch")
plt.ylabel("Train Error %")
plt.title("Error on MNIST Training Data 5 Layer Network Batch Norm")
plt.legend(title = "Initialization Method")
plt.savefig("BatchNorm5layerTrain.png")
files.download("BatchNorm5layerTrain.png")
plt.show()

plt.plot(epoch_axis, gaussStand_weight_norm_train_errors,'sk-' , label = 'Stand. Gaussian')
plt.plot(epoch_axis, gaussMod_weight_norm_train_errors,'dg-' , label = 'Modified Gaussian')

plt.plot(epoch_axis, reg_weight_norm_train_errors, '^b-', label = "Stand. Uniform Init")
plt.plot(epoch_axis, uniformMod_weight_norm_train_errors,'pm-' , label = 'Modified Uniform')

plt.xticks(np.arange(0, 20, step=2))
plt.xlabel("Epoch")
plt.ylabel("Train Error %")
plt.title("Error on MNIST Training Data 5 Layer Network Weight Norm")
plt.legend(title = "Initialization Method")
plt.savefig("WeightNorm5layerTrain.png")
files.download("WeightNorm5layerTrain.png")
plt.show()

plt.plot(epoch_axis, gaussStand_BW_norm_train_errors,'sk-' , label = 'Stand. Gaussian')
plt.plot(epoch_axis, gaussMod_BW_norm_train_errors,'dg-' , label = 'Modified Gaussian')

plt.plot(epoch_axis, reg_BW_norm_train_errors, '^b-', label = "Stand. Uniform Init")
plt.plot(epoch_axis, uniformMod_BW_norm_train_errors,'pm-' , label = 'Modified Uniform')

plt.xticks(np.arange(0, 20, step=2))
plt.xlabel("Epoch")
plt.ylabel("Train Error %")
plt.title("Error on MNIST Training Data 5 Layer Network Batch & Weight Norm")
plt.legend(title = "Initialization Method")
plt.savefig("BWNorm5layerTrain.png")
files.download("BWNorm5layerTrain.png")
plt.show()
