# -*- coding: utf-8 -*-
"""

This file generates a fourteen layer CNN with 10 linear layers in order to perform classification on the CIFAR10 dataset. The code compares results when the Network
contains batch normalization, weight normalization, batch and weight normalization, and no normalization.


Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11ukzqb5zJbki3UUWAaIzexDspxXsrqVN
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from random import randint
import time
import torchvision
import torchvision.datasets as datasets


from torch.nn.utils import weight_norm

from google.colab import files

import matplotlib.pyplot as plt
import numpy as np

#GPU info
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

#Memory management
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

def get_error(scores, labels): #calculate accuracy given network output and true labels

    bs = scores.size(0)
    predicted_labels = scores.argmax(dim=1)
    indicator = (predicted_labels == labels)
    num_matches=indicator.sum()

    return 1-num_matches.float()/bs

device = torch.device("cuda")

#Download CIFAR10 Data

batch_size = 128

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True ,transform=None)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True,transform=None)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

device= torch.device("cuda")
# device= torch.device("cpu")
print(device)

train_data = torch.Tensor(trainset.data)
train_data = train_data.permute(0, 3, 1, 2)
test_data = torch.Tensor(testset.data)
test_data = test_data.permute(0, 3, 1, 2)

train_label = torch.tensor(trainset.targets)
test_label = torch.tensor(testset.targets)

class fourteen_layer_convnet(nn.Module): #Define the network

    def __init__(self, weight_normalization = True):

        super().__init__()

        # block 1:
        self.conv1a = nn.Conv2d(3, 96,  kernel_size=3,  padding=1 )
        self.conv1b = nn.Conv2d(96, 96,  kernel_size=3,  padding=1 )
        self.pool1  = nn.MaxPool2d(2,2)

        self.dropout = nn.Dropout(.25)

        # block 2:
        self.conv2a = nn.Conv2d(96, 192, kernel_size=3, padding=1 )
        self.conv2b = nn.Conv2d(192, 192, kernel_size=3, padding=1 )
        self.pool2  = nn.MaxPool2d(2,2)

        # linear layers:
        self.linear1 = nn.Linear(12288, 200)
        self.linear1_batchnorm = nn.BatchNorm1d(200)

        self.linear2 = nn.Linear(200,100)
        self.linear2_batchnorm = nn.BatchNorm1d(100)

        self.linear3 = nn.Linear(100, 150)
        self.linear3_batchnorm = nn.BatchNorm1d(150)

        self.linear4 = nn.Linear(150, 100)
        self.linear4_batchnorm = nn.BatchNorm1d(100)

        self.linear5 = nn.Linear(100, 200)
        self.linear5_batchnorm = nn.BatchNorm1d(200)

        self.linear6 = nn.Linear(200, 150)
        self.linear6_batchnorm = nn.BatchNorm1d(150)

        self.linear7 = nn.Linear(150, 100)
        self.linear7_batchnorm = nn.BatchNorm1d(100)

        self.linear8 = nn.Linear(100, 100)
        self.linear8_batchnorm = nn.BatchNorm1d(100)

        self.linear9 = nn.Linear(100, 100)
        self.linear9_batchnorm = nn.BatchNorm1d(100)

        self.linear10 = nn.Linear(100, 10)

        if weight_normalization:
          self.conv1a = weight_norm(self.conv1a)
          self.conv1b = weight_norm(self.conv1b)

          self.conv2a = weight_norm(self.conv2a)
          self.conv2b = weight_norm(self.conv2b)

          self.linear1 = weight_norm(self.linear1)
          self.linear2 = weight_norm(self.linear2)
          self.linear3 = weight_norm(self.linear3)
          self.linear4 = weight_norm(self.linear4)
          self.linear5 = weight_norm(self.linear5)
          self.linear6 = weight_norm(self.linear6)
          self.linear7 = weight_norm(self.linear7)
          self.linear8 = weight_norm(self.linear8)
          self.linear9 = weight_norm(self.linear9)
          self.linear10 = weight_norm(self.linear10)


    def forward(self, x, batch_norm = True):

        # block 1:
        x = self.conv1a(x)
        x = F.relu(x)
        x = self.conv1b(x)
        x = F.relu(x)
        x = self.pool1(x)

        x = self.dropout(x)

        # block 2:
        x = self.conv2a(x)
        x = F.relu(x)
        x = self.conv2b(x)
        x = F.relu(x)
        x = self.pool2(x)

        x = self.dropout(x)

        # linear layers:
        x = x.view(-1, 12288)
        if batch_norm:
          x = self.linear1(x)
          x = self.linear1_batchnorm(x)
          x = F.relu(x)

          x = self.linear2(x)
          x = self.linear2_batchnorm(x)
          x = F.relu(x)

          x = self.linear3(x)
          x = self.linear3_batchnorm(x)
          x = F.relu(x)

          x = self.linear4(x)
          x = self.linear4_batchnorm(x)
          x = F.relu(x)

          x = self.linear5(x)
          x = self.linear5_batchnorm(x)
          x = F.relu(x)

          x = self.linear6(x)
          x = self.linear6_batchnorm(x)
          x = F.relu(x)

          x = self.linear7(x)
          x = self.linear7_batchnorm(x)
          x = F.relu(x)

          x = self.linear8(x)
          x = self.linear8_batchnorm(x)
          x = F.relu(x)

          x = self.linear9(x)
          x = self.linear9_batchnorm(x)
          x = F.relu(x)

          x = self.linear10(x)

        else:
          x = self.linear1(x)
          x = F.relu(x)
          x = self.linear2(x)
          x = F.relu(x)
          x = self.linear3(x)
          x = F.relu(x)
          x = self.linear4(x)
          x = F.relu(x)
          x = self.linear5(x)
          x = F.relu(x)
          x = self.linear6(x)
          x = F.relu(x)
          x = self.linear7(x)
          x = F.relu(x)
          x = self.linear8(x)
          x = F.relu(x)
          x = self.linear9(x)
          x = F.relu(x)
          x = self.linear10(x)

        return x



mean = train_data.mean()
std= train_data.std()

#net = net.to(device)
mean = mean.to(device)
std = std.to(device)

criterion = nn.CrossEntropyLoss()
my_lr=0.25
bs= 100
num_epochs = 20

def eval_on_test_set(batch_norm = True): #evaluate network on testing data

    running_error=0
    num_batches=0

    with torch.no_grad():

        for i in range(0,10000,bs):

            minibatch_data =  test_data[i:i+bs]
            minibatch_label= test_label[i:i+bs]

            minibatch_data=minibatch_data.to(device)
            minibatch_label=minibatch_label.to(device)

            inputs = minibatch_data.view(-1,3,32,32)


            # normalize the minibatch

            inputs = (minibatch_data - mean)/std
            inputs = inputs.float()

            scores=net( inputs, batch_norm )

            error = get_error( scores , minibatch_label)

            running_error += error.item()

            num_batches+=1


    total_error = running_error/num_batches
    print( 'error rate on test set =', total_error*100 ,'percent')
    return total_error

def train_the_net(my_lr, batch_norm = True): #train the network

  start=time.time()

  train_errors = []
  test_errors = []

  for epoch in range(1,num_epochs):

      # divide the learning rate by 2 at epoch 10, 14 and 18
      if epoch==10 or epoch == 14 or epoch==18:
          my_lr = my_lr / 2

      # create a new optimizer at the beginning of each epoch: give the current learning rate.
      optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )

      # set the running quatities to zero at the beginning of the epoch
      running_loss=0
      running_error=0
      num_batches=0

      # set the order in which to visit the image from the training set
      shuffled_indices=torch.randperm(50000)

      for count in range(0,50000,bs):

          # Set the gradients to zeros
          optimizer.zero_grad()

          # create a minibatch
          indices=shuffled_indices[count:count+bs]
          minibatch_data =  train_data[indices]
          minibatch_label=  train_label[indices]


          # send them to the gpu
          minibatch_data=minibatch_data.to(device)
          minibatch_label=minibatch_label.to(device)

          # normalize the minibatch
          inputs = (minibatch_data - mean)/std
          inputs = inputs.float()

          # tell Pytorch to start tracking all operations that will be done on "inputs"
          inputs.requires_grad_()



          # forward the minibatch through the net
          scores=net(inputs, batch_norm)


          # Compute the average of the losses of the data points in the minibatch
          loss =  criterion( scores , minibatch_label)

          # backward pass to compute dL/dU, dL/dV and dL/dW
          loss.backward()

          # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...
          optimizer.step()

          # COMPUTE STATS
          num_batches+=1
          with torch.no_grad():
              running_loss += loss.item()
              error = get_error( scores , minibatch_label)
              running_error += error.item()


      # compute stats for the full training set
      total_loss = running_loss/num_batches
      total_error = running_error/num_batches
      elapsed = (time.time()-start)/60

      train_errors.append(100*total_error)


      print('epoch=',epoch, '\t time=', elapsed,'min','\t lr=', my_lr  ,'\t loss=', total_loss , '\t error=', total_error*100 ,'percent')
      test_error = eval_on_test_set(batch_norm)
      test_errors.append(100*test_error)
      print(' ')


  return train_errors, test_errors

#Run network with weight and batch norm
net = fourteen_layer_convnet(weight_normalization = True)
net = net.to(device)
batch_weight_norm_train_errors, batch_weight_norm_test_errors = train_the_net(my_lr, batch_norm = True)

#Run network with weight norm
net = fourteen_layer_convnet(weight_normalization = True)
net = net.to(device)
weight_norm_train_errors, weight_norm_test_errors = train_the_net(my_lr, batch_norm = False)

#Run network with batch norm
net = fourteen_layer_convnet(weight_normalization = False)
net = net.to(device)
batch_norm_train_errors, batch_norm_test_errors = train_the_net(my_lr, batch_norm = True)

#Run network with no norm
net = fourteen_layer_convnet(weight_normalization = False)
net = net.to(device)
no_norm_train_errors, no_norm_test_errors = train_the_net(my_lr, batch_norm = False)

#Create and save plots
epoch_axis = np.arange(1, num_epochs)

plt.plot(epoch_axis, no_norm_train_errors,'sk-' , label = 'None')
plt.plot(epoch_axis, batch_norm_train_errors, '^b-', label = "BN")
plt.plot(epoch_axis, weight_norm_train_errors, 'dg-', label = 'WN')
plt.plot(epoch_axis, batch_weight_norm_train_errors, 'or-', label = "BN + WN")
plt.legend()
plt.xticks(np.arange(0, 20, step=2))
plt.xlabel("Epoch")
plt.ylabel("Train Error %")
plt.title("Error on CIFAR Training Data - 10 Layers")
#plt.caption("2 linear layers at end of CNN")
plt.savefig("CNN_10layerTrain.png")
files.download("CNN_10layerTrain.png")
plt.show()

plt.plot(epoch_axis, no_norm_test_errors,'sk-' , label = 'None')
plt.plot(epoch_axis, batch_norm_test_errors, '^b-', label = "BN")
plt.plot(epoch_axis, weight_norm_test_errors, 'dg-', label = 'WN')
plt.plot(epoch_axis, batch_weight_norm_test_errors, 'or-', label = "BN + WN")
plt.legend()
plt.xticks(np.arange(0, 20, step=2))
plt.xlabel("Epoch")
plt.ylabel("Test Error %")
plt.title("Error on CIFAR Test Data - 10 Layers")
#plt.caption("2 linear layers at end of CNN")
plt.savefig("CNN_10layerTest.png")
files.download("CNN_10layerTest.png")
plt.show()
