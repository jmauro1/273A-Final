# -*- coding: utf-8 -*-
"""

This file generates a nine layer CNN with 5 linear layers in order to perform classification on the CIFAR10 dataset. The code compares results when the Network
contains batch normalization, weight normalization, batch and weight normalization, and no normalization. The weights of the network are initialized using the
standard uniform distribution, standard Gaussian distribution, Xavier uniform distribution, and a modified Gaussian dist.

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rB7J6eisOzQoHKuACAkGGLcsEdgJN7-j
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from random import randint
import time
import torchvision
import torchvision.datasets as datasets


from torch.nn.utils import weight_norm

from google.colab import files

import matplotlib.pyplot as plt
import numpy as np

#Show GPU info
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

#Memory Management
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

def get_error(scores, labels): #calculate error from network output and groundtruth labels

    bs = scores.size(0)
    predicted_labels = scores.argmax(dim=1)
    indicator = (predicted_labels == labels)
    num_matches=indicator.sum()

    return 1-num_matches.float()/bs

device = torch.device("cuda")

#Download data
batch_size = 128

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True ,transform=None)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True,transform=None)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

device= torch.device("cuda")
# device= torch.device("cpu")
print(device)

train_data = torch.Tensor(trainset.data)
train_data = train_data.permute(0, 3, 1, 2)
test_data = torch.Tensor(testset.data)
test_data = test_data.permute(0, 3, 1, 2)

train_label = torch.tensor(trainset.targets)
test_label = torch.tensor(testset.targets)

class six_layer_convnet(nn.Module):

    def __init__(self, weight_normalization = True, Gauss_init_stand = False, Gauss_init_modified = False, uniform_init_modified = False):
       """
      Creates a five layer linear network
      weight_normalization specifies if weights should be dynamically normalized
      Gaussian_init_stand specifies if weights should be initialized by being sampled from Gaussian dist. with 0 mean and sd. 1
      Gaussian_init_stand specifies if weights should be initialized by being sampled from Gaussian dist. with 0 mean and sd. (2/n)^.5
      uniform_init_modified specifies if weights should be initialized by being sampled from Xavier Uniform dist.
      """

        super().__init__()

        # block 1:
        self.conv1a = nn.Conv2d(3, 96,  kernel_size=3,  padding=1 )
        self.conv1b = nn.Conv2d(96, 96,  kernel_size=3,  padding=1 )
        self.pool1  = nn.MaxPool2d(2,2)

        self.dropout = nn.Dropout(.25)

        # block 2:
        self.conv2a = nn.Conv2d(96, 192, kernel_size=3, padding=1 )
        self.conv2b = nn.Conv2d(192, 192, kernel_size=3, padding=1 )
        self.pool2  = nn.MaxPool2d(2,2)

        # linear layers:
        if Gauss_init_stand:
          self.linear1 = nn.Linear(12288, 200)
          nn.init.normal_(self.linear1.weight, 0, 1/(5**.5))
          self.linear1_batchnorm = nn.BatchNorm1d(200)

          self.linear2 = nn.Linear(200,100)
          nn.init.normal_(self.linear2.weight, 0, 1/(5**.5))
          self.linear2_batchnorm = nn.BatchNorm1d(100)

          self.linear3 = nn.Linear(100, 150)
          nn.init.normal_(self.linear3.weight, 0, 1/(5**.5))
          self.linear3_batchnorm = nn.BatchNorm1d(150)

          self.linear4 = nn.Linear(150, 100)
          nn.init.normal_(self.linear4.weight, 0, 1/(5**.5))
          self.linear4_batchnorm = nn.BatchNorm1d(100)

          self.linear5 = nn.Linear(100, 10)
          nn.init.normal_(self.linear5.weight, 0, 1/(5**.5))

        elif Gauss_init_modified:
          self.linear1 = nn.Linear(12288, 200)
          nn.init.normal_(self.linear1.weight, 0, (2/(self.linear1.weight.shape[0]*self.linear1.weight.shape[1]))**.5)
          self.linear1_batchnorm = nn.BatchNorm1d(200)

          self.linear2 = nn.Linear(200,100)
          nn.init.normal_(self.linear2.weight, 0, (2/(self.linear2.weight.shape[0]*self.linear2.weight.shape[1]))**.5)
          self.linear2_batchnorm = nn.BatchNorm1d(100)

          self.linear3 = nn.Linear(100, 150)
          nn.init.normal_(self.linear3.weight, 0, (2/(self.linear3.weight.shape[0]*self.linear3.weight.shape[1]))**.5)
          self.linear3_batchnorm = nn.BatchNorm1d(150)

          self.linear4 = nn.Linear(150, 100)
          nn.init.normal_(self.linear4.weight, 0, (2/(self.linear4.weight.shape[0]*self.linear4.weight.shape[1]))**.5)
          self.linear4_batchnorm = nn.BatchNorm1d(100)

          self.linear5 = nn.Linear(100, 10)
          nn.init.normal_(self.linear5.weight, 0, (2/(self.linear5.weight.shape[0]*self.linear5.weight.shape[1]))**.5)

        elif uniform_init_modified:
          self.linear1 = nn.Linear(12288, 200)
          nn.init.xavier_uniform_(self.linear1.weight)
          self.linear1_batchnorm = nn.BatchNorm1d(200)

          self.linear2 = nn.Linear(200,100)
          nn.init.xavier_uniform_(self.linear2.weight)
          self.linear2_batchnorm = nn.BatchNorm1d(100)

          self.linear3 = nn.Linear(100, 150)
          nn.init.xavier_uniform_(self.linear3.weight)
          self.linear3_batchnorm = nn.BatchNorm1d(150)

          self.linear4 = nn.Linear(150, 100)
          nn.init.xavier_uniform_(self.linear4.weight)
          self.linear4_batchnorm = nn.BatchNorm1d(100)

          self.linear5 = nn.Linear(100, 10)
          nn.init.xavier_uniform_(self.linear5.weight)


        else:
          self.linear1 = nn.Linear(12288, 200)
          self.linear1_batchnorm = nn.BatchNorm1d(200)
          self.linear2 = nn.Linear(200,100)
          self.linear2_batchnorm = nn.BatchNorm1d(100)
          self.linear3 = nn.Linear(100, 150)
          self.linear3_batchnorm = nn.BatchNorm1d(150)
          self.linear4 = nn.Linear(150, 100)
          self.linear4_batchnorm = nn.BatchNorm1d(100)
          self.linear5 = nn.Linear(100, 10)

        if weight_normalization:
          self.conv1a = weight_norm(self.conv1a)
          self.conv1b = weight_norm(self.conv1b)

          self.conv2a = weight_norm(self.conv2a)
          self.conv2b = weight_norm(self.conv2b)

          self.linear1 = weight_norm(self.linear1)
          self.linear2 = weight_norm(self.linear2)
          self.linear3 = weight_norm(self.linear3)
          self.linear4 = weight_norm(self.linear4)
          self.linear5 = weight_norm(self.linear5)


    def forward(self, x, batch_norm = True):

        # block 1:
        x = self.conv1a(x)
        x = F.relu(x)
        x = self.conv1b(x)
        x = F.relu(x)
        x = self.pool1(x)

        x = self.dropout(x)

        # block 2:
        x = self.conv2a(x)
        x = F.relu(x)
        x = self.conv2b(x)
        x = F.relu(x)
        x = self.pool2(x)

        x = self.dropout(x)

        # linear layers:
        x = x.view(-1, 12288)
        if batch_norm:
          x = self.linear1(x)
          x = self.linear1_batchnorm(x)
          x = F.relu(x)

          x = self.linear2(x)
          x = self.linear2_batchnorm(x)
          x = F.relu(x)

          x = self.linear3(x)
          x = self.linear3_batchnorm(x)
          x = F.relu(x)

          x = self.linear4(x)
          x = self.linear4_batchnorm(x)
          x = F.relu(x)

          x = self.linear5(x)



        else:
          x = self.linear1(x)
          x = F.relu(x)
          x = self.linear2(x)
          x = F.relu(x)
          x = self.linear3(x)
          x = F.relu(x)
          x = self.linear4(x)
          x = F.relu(x)
          x = self.linear5(x)

        return x

mean = train_data.mean()
std= train_data.std()

mean = mean.to(device)
std = std.to(device)

criterion = nn.CrossEntropyLoss()
my_lr=0.25
bs= 100
num_epochs = 40

def eval_on_test_set(epoch, batch_norm = True): #evaluate performance on test data

    running_error=0
    num_batches=0

    with torch.no_grad():

        for i in range(0,10000,bs):

            minibatch_data =  test_data[i:i+bs]
            minibatch_label= test_label[i:i+bs]

            minibatch_data=minibatch_data.to(device)
            minibatch_label=minibatch_label.to(device)

            inputs = minibatch_data.view(-1,3,32,32)

            inputs = (inputs - mean)/std

            scores=net( inputs, batch_norm )

            error = get_error( scores , minibatch_label)

            running_error += error.item()

            num_batches+=1


    total_error = running_error/num_batches
    if epoch % 5  ==0:
      print( 'error rate on test set =', total_error*100 ,'percent')
    return total_error

def train_the_net(my_lr, batch_norm = True): #train the network

  start=time.time()

  train_errors = []
  test_errors = []

  for epoch in range(1,num_epochs):

      # divide the learning rate by 2 at epoch 10, 14 and 18
      if epoch==10 or epoch == 14 or epoch==18:
          my_lr = my_lr / 2

      # create a new optimizer at the beginning of each epoch: give the current learning rate.
      optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )

      # set the running quatities to zero at the beginning of the epoch
      running_loss=0
      running_error=0
      num_batches=0

      # set the order in which to visit the image from the training set
      shuffled_indices=torch.randperm(50000)

      for count in range(0,50000,bs):

          # Set the gradients to zeros
          optimizer.zero_grad()

          # create a minibatch
          indices=shuffled_indices[count:count+bs]
          minibatch_data =  train_data[indices]
          minibatch_label=  train_label[indices]


          # send them to the gpu
          minibatch_data=minibatch_data.to(device)
          minibatch_label=minibatch_label.to(device)


          # normalize the minibatch
          inputs = (minibatch_data - mean)/std
          inputs = inputs.float()

          # tell Pytorch to start tracking all operations that will be done on "inputs"
          inputs.requires_grad_()



          # forward the minibatch through the net
          scores=net(inputs, batch_norm)


          # Compute the average of the losses of the data points in the minibatch
          loss =  criterion( scores , minibatch_label)

          # backward pass to compute dL/dU, dL/dV and dL/dW
          loss.backward()

          # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...
          optimizer.step()

          # COMPUTE STATS
          num_batches+=1
          with torch.no_grad():
              running_loss += loss.item()
              error = get_error( scores , minibatch_label)
              running_error += error.item()


      # compute stats for the full training set
      total_loss = running_loss/num_batches
      total_error = running_error/num_batches
      elapsed = (time.time()-start)/60

      train_errors.append(100*total_error)

      if epoch % 5 == 0:
        print('epoch=',epoch, '\t time=', elapsed,'min','\t lr=', my_lr  ,'\t loss=', total_loss , '\t error=', total_error*100 ,'percent')
      test_error = eval_on_test_set(epoch, batch_norm)
      test_errors.append(100*test_error)
      print(' ')


  return train_errors, test_errors



#Run networks with batch and weight norm
net = six_layer_convnet(weight_normalization = True, Gauss_init_stand = True)
net = net.to(device)
gaussStand_BW_norm_train_errors, gaussStand_BW_norm_test_errors = train_the_net(my_lr, batch_norm = True)

print(" ")

net = six_layer_convnet(weight_normalization = True, Gauss_init_modified = True)
net = net.to(device)
gaussMod_BW_norm_train_errors, gaussMod_BW_norm_test_errors = train_the_net(my_lr, batch_norm = True)

print(" ")

net = six_layer_convnet(weight_normalization = True, uniform_init_modified = True)
net = net.to(device)
uniformMod_BW_norm_train_errors, uniformMod_BW_norm_test_errors = train_the_net(my_lr, batch_norm = True)

print(" ")

net = six_layer_convnet(weight_normalization = True)
net = net.to(device)
reg_BW_norm_train_errors, reg_BW_norm_test_errors = train_the_net(my_lr, batch_norm = True)

#Run networks with batch norm
net = six_layer_convnet(weight_normalization = False, Gauss_init_stand = True)
net = net.to(device)
gaussStand_batch_norm_train_errors, gaussStand_batch_norm_test_errors = train_the_net(my_lr, batch_norm = True)

print(" ")

net = six_layer_convnet(weight_normalization = False, Gauss_init_modified = True)
net = net.to(device)
gaussMod_batch_norm_train_errors, gaussMod_batch_norm_test_errors = train_the_net(my_lr, batch_norm = True)

print(" ")

net = six_layer_convnet(weight_normalization = False, uniform_init_modified = True)
net = net.to(device)
uniformMod_batch_norm_train_errors, uniformMod_batch_norm_test_errors = train_the_net(my_lr, batch_norm = True)

print(" ")

net = six_layer_convnet(weight_normalization = False)
net = net.to(device)
reg_batch_norm_train_errors, reg_batch_norm_test_errors = train_the_net(my_lr, batch_norm = True)

#Run networks with weight norm

net = six_layer_convnet(weight_normalization = True, Gauss_init_stand = True)
net = net.to(device)
gaussStand_weight_norm_train_errors, gaussStand_weight_norm_test_errors = train_the_net(my_lr, batch_norm = False)

print(" ")

net = six_layer_convnet(weight_normalization = True, Gauss_init_modified = True)
net = net.to(device)
gaussMod_weight_norm_train_errors, gaussMod_weight_norm_test_errors = train_the_net(my_lr, batch_norm = False)

print(" ")

net = six_layer_convnet(weight_normalization = True, uniform_init_modified = True)
net = net.to(device)
uniformMod_weight_norm_train_errors, uniformMod_weight_norm_test_errors = train_the_net(my_lr, batch_norm = False)

print(" ")

net = six_layer_convnet(weight_normalization = True)
net = net.to(device)
reg_weight_norm_train_errors, reg_weight_norm_test_errors = train_the_net(my_lr, batch_norm = False)

#Run networks with no norm

net = six_layer_convnet(weight_normalization = False, Gauss_init_stand = True)
net = net.to(device)
gaussStand_no_norm_train_errors, gaussStand_no_norm_test_errors = train_the_net(my_lr, batch_norm = False)

print(" ")

net = six_layer_convnet(weight_normalization = False, Gauss_init_modified = True)
net = net.to(device)
gaussMod_no_norm_train_errors, gaussMod_no_norm_test_errors = train_the_net(my_lr, batch_norm = False)

print(" ")

net = six_layer_convnet(weight_normalization = False, uniform_init_modified = True)
net = net.to(device)
uniformMod_no_norm_train_errors, uniformMod_no_norm_test_errors = train_the_net(my_lr, batch_norm = False)

print(" ")

net = six_layer_convnet(weight_normalization = False)
net = net.to(device)
reg_no_norm_train_errors, reg_no_norm_test_errors = train_the_net(my_lr, batch_norm = False)



#Create plots

epoch_axis = np.arange(1, num_epochs)



plt.plot(epoch_axis, gaussStand_no_norm_train_errors,'sk-' , label = 'Stand. Gaussian')
plt.plot(epoch_axis, gaussMod_no_norm_train_errors,'dg-' , label = 'Modified Gaussian')

plt.plot(epoch_axis, reg_no_norm_train_errors, '^b-', label = "Stand. Uniform Init")
plt.plot(epoch_axis, uniformMod_no_norm_train_errors,'pm-' , label = 'Modified Uniform')

plt.xticks(np.arange(0, num_epochs , step=2))
plt.xlabel("Epoch")
plt.ylabel("Train Error %")
plt.title("Error on CIFAR Training Data 5 Layer Network No Normalization")
plt.legend(title = "Initialization Method")
plt.savefig("NoNormCIFAR5layerTrain.png")
files.download("NoNormCIFAR5layerTrain.png")
plt.show()

plt.plot(epoch_axis, gaussStand_batch_norm_train_errors,'sk-' , label = 'Stand. Gaussian')
plt.plot(epoch_axis, gaussMod_batch_norm_train_errors,'dg-' , label = 'Modified Gaussian')

plt.plot(epoch_axis, reg_batch_norm_train_errors, '^b-', label = "Stand. Uniform Init")
plt.plot(epoch_axis, uniformMod_batch_norm_train_errors,'pm-' , label = 'Modified Uniform')

plt.xticks(np.arange(0, num_epochs, step=2))
plt.xlabel("Epoch")
plt.ylabel("Train Error %")
plt.title("Error on CIFAR Training Data 5 Layer Network Batch Norm")
plt.legend(title = "Initialization Method")
plt.savefig("BatchNormCIFAR5layerTrain.png")
files.download("BatchNormCIFAR5layerTrain.png")
plt.show()

plt.plot(epoch_axis, gaussStand_weight_norm_train_errors,'sk-' , label = 'Stand. Gaussian')
plt.plot(epoch_axis, gaussMod_weight_norm_train_errors,'dg-' , label = 'Modified Gaussian')

plt.plot(epoch_axis, reg_weight_norm_train_errors, '^b-', label = "Stand. Uniform Init")
plt.plot(epoch_axis, uniformMod_weight_norm_train_errors,'pm-' , label = 'Modified Uniform')

plt.xticks(np.arange(0, num_epochs, step=2))
plt.xlabel("Epoch")
plt.ylabel("Train Error %")
plt.title("Error on CIFAR Training Data 5 Layer Network Weight Norm")
plt.legend(title = "Initialization Method")
plt.savefig("WeightNormCIFARlayerTrain.png")
files.download("WeightNormCIFARlayerTrain.png")
plt.show()

plt.plot(epoch_axis, gaussStand_BW_norm_train_errors,'sk-' , label = 'Stand. Gaussian')
plt.plot(epoch_axis, gaussMod_BW_norm_train_errors,'dg-' , label = 'Modified Gaussian')

plt.plot(epoch_axis, reg_BW_norm_train_errors, '^b-', label = "Stand. Uniform Init")
plt.plot(epoch_axis, uniformMod_BW_norm_train_errors,'pm-' , label = 'Modified Uniform')

plt.xticks(np.arange(0, num_epochs, step=2))
plt.xlabel("Epoch")
plt.ylabel("Train Error %")
plt.title("Error on CIFAR Training Data 5 Layer Network Batch & Weight Norm")
plt.legend(title = "Initialization Method")
plt.savefig("BWNormCIFAR5layerTrain.png")
files.download("BWNormCIFAR5layerTrain.png")
plt.show()
